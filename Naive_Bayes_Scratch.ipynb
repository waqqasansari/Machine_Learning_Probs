{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive_Bayes_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQKXjYlFXCt8Bh68XESbY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqqasansari/Machine_Learning_Probs/blob/master/Naive_Bayes_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh3JAjI3LiN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "cf183fce-653e-4500-a772-79f1fefad129"
      },
      "source": [
        "pip install pdp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdp\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/f9/5e4886980fd2a86013055142f9f3c9f94d3205495a0603a55d5eae32ac9d/pdp-0.3.0.tar.gz\n",
            "Building wheels for collected packages: pdp\n",
            "  Building wheel for pdp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdp: filename=pdp-0.3.0-cp36-none-any.whl size=6603 sha256=99339de78856319006e99cd8be104fbe92f9d5d1a163baef4dc3ee75ce160032\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/fb/5e/afb783110614b3c1a4187e6f83e4f4ea0088fbdb82921013a2\n",
            "Successfully built pdp\n",
            "Installing collected packages: pdp\n",
            "Successfully installed pdp-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD3pECtoK6xB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pdp\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fFRhoi_Lnsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5fb17888-7572-4d2f-f764-be1541c23439"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWnoV7umLuma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# avoid assumptions about the length of all_positive_tweets\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q76cXklL1hJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2a28fb90-5ee4-4536-e506-23e4c39b604d"
      },
      "source": [
        "print(len(train_y))\n",
        "print(test_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n",
            "[1. 1. 1. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qrpng_QL4uE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAAUI9QaMWUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_tweets(result, tweets, ys):\n",
        "    '''\n",
        "    Input:\n",
        "        result: a dictionary that will be used to map each pair to its frequency\n",
        "        tweets: a list of tweets\n",
        "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "        result: a dictionary mapping each pair to its frequency\n",
        "    '''\n",
        "    for y, tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            # define the key, which is the word and label tuple\n",
        "            pair = (word, y)\n",
        "\n",
        "            # if the key exists in the dictionary, increment the count\n",
        "            if pair in result:\n",
        "                result[pair] += 1\n",
        "\n",
        "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
        "            else:\n",
        "                result[pair] = 1\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSJd2DZyMaT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cb7a201-2429-4e67-d558-0dde70641b72"
      },
      "source": [
        "# Testing your function\n",
        "result = {}\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "count_tweets(result, tweets, ys)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('sad', 0): 1, ('tire', 0): 2, ('trick', 0): 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4H4fYDMPACX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the freqs dictionary for later uses\n",
        "freqs = count_tweets({}, train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kk9XovBRkjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lookup(freqs, word, label):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        word: the word to look up\n",
        "        label: the label corresponding to the word\n",
        "    Output:\n",
        "        n: the number of times the word with its corresponding label appears.\n",
        "    '''\n",
        "    n = 0  # freqs.get((word, label), 0)\n",
        "\n",
        "    pair = (word, label)\n",
        "    if (pair in freqs):\n",
        "        n = freqs[pair]\n",
        "\n",
        "    return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm5goMtVPHWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "\n",
        "  loglikelihood = {}\n",
        "  logprior = 0\n",
        "\n",
        "  vocab = set([pair[0] for pair in freqs.keys()])\n",
        "  V = len(vocab)\n",
        "\n",
        "  N_pos = N_neg = 0\n",
        "  for pair in freqs.keys():\n",
        "    if pair[1] > 0:\n",
        "      N_pos += freqs[pair]\n",
        "    else:\n",
        "      N_neg += freqs[pair]\n",
        "\n",
        "  D = len(train_y)\n",
        "\n",
        "  D_pos = len(list(filter(lambda x: x > 0, train_y)))\n",
        "  D_neg = len(list(filter(lambda x: x <= 0, train_y)))\n",
        "\n",
        "  logprior = np.log(D_pos) - np.log(D_neg)\n",
        "\n",
        "  for word in vocab:\n",
        "\n",
        "    freq_pos = lookup(freqs,word,1)\n",
        "    freq_neg = lookup(freqs,word,0)\n",
        "\n",
        "    p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
        "    p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
        "\n",
        "    loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "\n",
        "  return logprior, loglikelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtQzmz5eSELI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d544c836-b8c9-425a-f93b-b42c83303bfc"
      },
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "9089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLVo4yurVY4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "  '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        logprior: a number\n",
        "        loglikelihood: a dictionary of words mapping to numbers\n",
        "    Output:\n",
        "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
        "\n",
        "    '''\n",
        "  word_l = process_tweet(tweet)\n",
        "\n",
        "  p = 0\n",
        "  p += logprior\n",
        "\n",
        "  for word in word_l:\n",
        "    if word in loglikelihood:\n",
        "      p += loglikelihood[word]\n",
        "\n",
        "  return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwKbKiijVx2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9051bb81-6782-40dc-af4a-728c0b9840fc"
      },
      "source": [
        "# Experiment with your own tweet.\n",
        "my_tweet = 'She smiled.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The expected output is 1.5740278623499175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoOAhIwMV3nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "\n",
        "  accuracy = 0\n",
        "\n",
        "  y_hats = []\n",
        "\n",
        "  for tweet in test_x:\n",
        "\n",
        "    if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "      y_hat_i = 1\n",
        "    else:\n",
        "\n",
        "      y_hat_i = 0\n",
        "\n",
        "    y_hats.append(y_hat_i)\n",
        "\n",
        "  error = np.mean(np.absolute(y_hats-test_y))\n",
        "\n",
        "  accuracy = (1 - error)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH3acbqqWc8V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efc143b2-7552-4b6f-89d8-793bac174331"
      },
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy = 0.9940\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}